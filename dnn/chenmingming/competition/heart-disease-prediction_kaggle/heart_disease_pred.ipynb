{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 241 entries, 0 to 240\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       241 non-null    int64  \n",
      " 1   sex       241 non-null    int64  \n",
      " 2   cp        241 non-null    int64  \n",
      " 3   trestbps  241 non-null    int64  \n",
      " 4   chol      241 non-null    int64  \n",
      " 5   fbs       241 non-null    int64  \n",
      " 6   restecg   241 non-null    int64  \n",
      " 7   thalach   241 non-null    int64  \n",
      " 8   exang     241 non-null    int64  \n",
      " 9   oldpeak   241 non-null    float64\n",
      " 10  slope     241 non-null    int64  \n",
      " 11  ca        241 non-null    int64  \n",
      " 12  thal      241 non-null    int64  \n",
      " 13  target    241 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 26.5 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62 entries, 0 to 61\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       62 non-null     int64  \n",
      " 1   sex       62 non-null     int64  \n",
      " 2   cp        62 non-null     int64  \n",
      " 3   trestbps  62 non-null     int64  \n",
      " 4   chol      62 non-null     int64  \n",
      " 5   fbs       62 non-null     int64  \n",
      " 6   restecg   62 non-null     int64  \n",
      " 7   thalach   62 non-null     int64  \n",
      " 8   exang     62 non-null     int64  \n",
      " 9   oldpeak   62 non-null     float64\n",
      " 10  slope     62 non-null     int64  \n",
      " 11  ca        62 non-null     int64  \n",
      " 12  thal      62 non-null     int64  \n",
      "dtypes: float64(1), int64(12)\n",
      "memory usage: 6.4 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": "target      1.000000\ncp          0.457688\nexang       0.453784\nca          0.408107\nthalach     0.390346\noldpeak     0.389787\nslope       0.334991\nthal        0.324611\nsex         0.281272\nage         0.242338\nrestecg     0.196018\nchol        0.170592\ntrestbps    0.154086\nfbs         0.035450\nName: target, dtype: float64"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "train.info()\n",
    "test.info()\n",
    "abs(train.corr()['target']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "[37 41 56 44 52 57 54 48 64 50 66 43 69 42 61 71 59 65 46 51 45 47 53 63\n",
      " 58 35 62 29 55 60 68 39 34 67 74 49 76 70 38 77 40]\n",
      "sex\n",
      "[1 0]\n",
      "cp\n",
      "[2 1 0 3]\n",
      "trestbps\n",
      "[130 140 120 172 150 110 160 125 142 135 155 104 138 128 108 134 122 115\n",
      " 118 100 124  94 112 102 152 101 132 178 129 136 106 156 170 117 145 180\n",
      " 165 192 144 123 126 154 148 114 164]\n",
      "chol\n",
      "[250 204 294 263 199 168 239 275 211 219 226 247 233 243 302 212 177 273\n",
      " 304 232 269 360 308 245 208 235 257 216 234 141 252 201 222 260 303 265\n",
      " 309 186 203 183 220 209 258 227 261 221 205 318 298 277 197 214 248 255\n",
      " 207 223 160 394 315 270 195 240 196 244 254 126 313 262 215 193 271 268\n",
      " 267 210 295 178 242 180 228 149 253 342 157 175 286 229 256 224 206 230\n",
      " 276 353 225 330 290 266 172 305 188 282 185 326 274 164 307 249 341 407\n",
      " 217 174 281 288 289 246 322 299 300 293 184 409 283 259 200 327 237 319\n",
      " 166 218 335 169 187 176 241 264 236]\n",
      "fbs\n",
      "[0 1]\n",
      "restecg\n",
      "[1 0 2]\n",
      "thalach\n",
      "[187 172 153 173 162 174 160 139 144 158 114 171 151 179 178 137 157 140\n",
      " 152 170 165 148 142 180 156 115 175 186 185 159 130 190 132 182 143 163\n",
      " 147 154 202 161 166 164 184 122 168 169 138 111 145 194 131 133 155 167\n",
      " 192 121  96 126 105 181 116 149 150 125 108 129 112 128 109 113  99 177\n",
      " 141 146 136 127 103 124  88 120 195  95 117  71 118 134  90 123]\n",
      "exang\n",
      "[0 1]\n",
      "oldpeak\n",
      "[3.5 1.4 1.3 0.  0.5 1.6 1.2 0.2 1.8 2.6 1.5 0.4 1.  0.8 3.  0.6 2.4 0.1\n",
      " 1.9 4.2 1.1 2.  0.7 0.3 0.9 2.3 3.6 3.2 2.2 2.8 3.4 6.2 4.  5.6 2.1 4.4]\n",
      "slope\n",
      "[0 2 1]\n",
      "ca\n",
      "[0 2 1 4 3]\n",
      "thal\n",
      "[2 3 0 1]\n",
      "target\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "for col in train.columns:\n",
    "    print(col)\n",
    "    print(train[col].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 241 entries, 0 to 240\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       241 non-null    int64  \n",
      " 1   sex       241 non-null    int64  \n",
      " 2   cp        241 non-null    object \n",
      " 3   trestbps  241 non-null    int64  \n",
      " 4   chol      241 non-null    int64  \n",
      " 5   fbs       241 non-null    int64  \n",
      " 6   restecg   241 non-null    object \n",
      " 7   thalach   241 non-null    int64  \n",
      " 8   exang     241 non-null    int64  \n",
      " 9   oldpeak   241 non-null    float64\n",
      " 10  slope     241 non-null    object \n",
      " 11  ca        241 non-null    object \n",
      " 12  thal      241 non-null    object \n",
      " 13  target    241 non-null    int64  \n",
      "dtypes: float64(1), int64(8), object(5)\n",
      "memory usage: 26.5+ KB\n"
     ]
    }
   ],
   "source": [
    "object_cols = ['cp', 'restecg', 'slope', 'ca', 'thal']\n",
    "def strfeatures(data):\n",
    "    data_ = data.copy()\n",
    "    for col in object_cols:\n",
    "        data_[col] = data_[col].astype(str)\n",
    "    return data_\n",
    "\n",
    "train_ = strfeatures(train)\n",
    "test_ = strfeatures(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def num_cat_split(data):\n",
    "    s = (data.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "    num_cols = list(set(data.columns)-set(object_cols))\n",
    "    return num_cols, object_cols\n",
    "\n",
    "num_cols, object_cols = num_cat_split(train_)\n",
    "num_cols.remove('target')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# 本地测试，分成抽样，分割训练集，验证集\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "splt = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=1)\n",
    "for train_idx, valid_idx in splt.split(train_, train_['target']):\n",
    "    train_part = train_.loc[train_idx]\n",
    "    valid_part = train_.loc[valid_idx]\n",
    "\n",
    "train_part_y = train_part['target']\n",
    "valid_part_y = valid_part['target']\n",
    "train_part = train_part.drop(['target'], axis=1)\n",
    "valid_part = valid_part.drop(['target'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_name):\n",
    "        self.attribute_name = attribute_name\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_name].values\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_cols)),\n",
    "    # ('imputer', SimpleImputer(strategy='median')),\n",
    "    # ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(object_cols)),\n",
    "    ('cat_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('cat_pipeline', cat_pipeline)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "{'model__max_iter': 10000}\n",
      "accuracy score:  0.4489795918367347\n",
      "Fitting 3 folds for each of 35 candidates, totalling 105 fits\n",
      "{'model__leaf_size': 3, 'model__n_neighbors': 3}\n",
      "accuracy score:  0.5306122448979592\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "{'model__C': 0.1, 'model__max_iter': 50000, 'model__penalty': 'l2'}\n",
      "accuracy score:  0.8979591836734694\n",
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n",
      "{'model__C': 1, 'model__degree': 5, 'model__kernel': 'poly'}\n",
      "accuracy score:  0.6326530612244898\n",
      "Fitting 3 folds for each of 135 candidates, totalling 405 fits\n",
      "{'model__max_depth': 5, 'model__max_features': 5, 'model__n_estimators': 250}\n",
      "accuracy score:  0.8775510204081632\n",
      "Fitting 3 folds for each of 7776 candidates, totalling 23328 fits\n",
      "{'model__learning_rate': 0.05, 'model__max_depth': 7, 'model__max_features': 20, 'model__min_samples_leaf': 10, 'model__min_samples_split': 40, 'model__n_estimators': 150, 'model__subsample': 0.5}\n",
      "accuracy score:  0.8163265306122449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    3.6s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  36 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed:   25.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1316 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2848 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 4490 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done 5984 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-1)]: Done 7672 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 12528 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 18040 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 23328 out of 23328 | elapsed:  3.3min finished\n"
     ]
    }
   ],
   "source": [
    "# 本地测试\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "lr = LogisticRegression()\n",
    "svc = SVC()\n",
    "gbdt = GradientBoostingClassifier()\n",
    "perceptron = Perceptron()\n",
    "\n",
    "models = [perceptron, knn, lr, svc, rf, gbdt]\n",
    "param_grid_list = [\n",
    "    # perceptron\n",
    "    [{\n",
    "        'model__max_iter' : [10000, 5000]\n",
    "    }],\n",
    "    # knn\n",
    "    [{\n",
    "        'model__n_neighbors' : [3,5,10,15,35],\n",
    "        'model__leaf_size' : [3,5,10,20,30,40,50]\n",
    "    }],\n",
    "    # lr\n",
    "    [{\n",
    "        'model__penalty' : ['l1', 'l2'],\n",
    "        'model__C' : [0.05, 0.1, 0.2, 0.5, 1, 1.2],\n",
    "        'model__max_iter' : [50000]\n",
    "    }],\n",
    "    # svc\n",
    "    [{\n",
    "        'model__degree' : [3, 5, 7],\n",
    "        'model__C' : [0.2, 0.5, 1, 1.2, 1.5],\n",
    "        'model__kernel' : ['rbf', 'sigmoid', 'poly']\n",
    "    }],\n",
    "    # rf\n",
    "    [{\n",
    "    #     'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'model__n_estimators' : [100,200,250,300,350],\n",
    "        'model__max_features' : [5,8, 10, 12, 15, 20, 30, 40, 50],\n",
    "        'model__max_depth' : [3,5,7]\n",
    "    }],\n",
    "    # gbdt\n",
    "    [{\n",
    "        'model__learning_rate' : [0.02, 0.05, 0.1, 0.2],\n",
    "        'model__n_estimators' : [30, 50, 100, 150],\n",
    "        'model__max_features' : [5, 8, 10,20,30,40],\n",
    "        'model__max_depth' : [3,5,7],\n",
    "        'model__min_samples_split' : [10, 20,40],\n",
    "        'model__min_samples_leaf' : [5,10,20],\n",
    "        'model__subsample' : [0.5, 0.8, 1]\n",
    "    }],\n",
    "]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    pipe = Pipeline([\n",
    "        ('preparation', full_pipeline),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    grid_search = GridSearchCV(pipe, param_grid_list[i], cv=3,\n",
    "                               scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "    grid_search.fit(train_part, train_part_y)\n",
    "    print(grid_search.best_params_)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    pred = final_model.predict(valid_part)\n",
    "    print('accuracy score: ', accuracy_score(valid_part_y, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 20 is smaller than n_iter=100. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of 100 | elapsed:    1.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   30.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "{'model__penalty': 'l2', 'model__max_iter': 50000, 'model__C': 0.3911111111111111}\n",
      "LogisticRegression() \n",
      "FINISH !!!\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "# 全量数据训练，提交测试\n",
    "# 采用随机参数搜索\n",
    "y_train = train_['target']\n",
    "X_train = train_.drop(['target'], axis=1)\n",
    "X_test = test_\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import numpy as np\n",
    "\n",
    "select_model = [lr, rf, gbdt]\n",
    "name = ['lr', 'rf', 'gbdt']\n",
    "param_distribs = [\n",
    "    # lr\n",
    "    [{\n",
    "        'model__penalty' : ['l1', 'l2'],\n",
    "        'model__C' : np.linspace(0.01, 0.5, 10),\n",
    "        'model__max_iter' : [50000]\n",
    "    }],\n",
    "    # rf\n",
    "    [{\n",
    "    #     'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'model__n_estimators' : randint(low=50, high=500),\n",
    "        'model__max_features' : randint(low=3, high=30),\n",
    "        'model__max_depth' : randint(low=2, high=20)\n",
    "    }],\n",
    "    # gbdt\n",
    "    [{\n",
    "        'model__learning_rate' : np.linspace(0.01, 0.3, 10),\n",
    "        'model__n_estimators' : randint(low=30, high=500),\n",
    "        'model__max_features' : randint(low=5, high=50),\n",
    "        'model__max_depth' : randint(low=3, high=20),\n",
    "        'model__min_samples_split' : randint(low=10, high=100),\n",
    "        'model__min_samples_leaf' : randint(low=3, high=50),\n",
    "        'model__subsample' : np.linspace(0.5, 1.5, 10)\n",
    "    }],\n",
    "]\n",
    "\n",
    "for i, model in enumerate(select_model):\n",
    "    pipe = Pipeline([\n",
    "        ('preparation', full_pipeline),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    rand_search = RandomizedSearchCV(pipe, param_distributions=param_distribs[i], cv=5,\n",
    "                                     n_iter=100, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "    rand_search.fit(X_train, y_train)\n",
    "    print(rand_search.best_params_)\n",
    "    final_model = rand_search.best_estimator_\n",
    "    pred = final_model.predict(X_test)\n",
    "    print(model,\"\\nFINISH !!!\")\n",
    "    res = pd.DataFrame()\n",
    "    res['Id'] = range(0,62,1)\n",
    "    res['Prediction'] = pred\n",
    "    res.to_csv('{}_pred.csv'.format(name[i]), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}