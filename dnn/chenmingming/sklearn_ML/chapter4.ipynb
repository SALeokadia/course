{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n",
    "X=[\n",
    "    {'city':'Beijing'},\n",
    "    {'city':'Guangzhou'},\n",
    "    {'city':'Shanghai'}\n",
    "]\n",
    "print(onehot_encoder.fit_transform(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "onehot_encoder = OneHotEncoder()\n",
    "X=[\n",
    "    {'city':'Beijing'},\n",
    "    {'city':'Guangzhou'},\n",
    "    {'city':'Shanghai'}\n",
    "]\n",
    "X = pd.DataFrame(X)\n",
    "print(onehot_encoder.fit_transform(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [4.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "X=[\n",
    "    {'city':1},\n",
    "    {'city':4},\n",
    "    {'city':5}\n",
    "]\n",
    "onehot_encoder = DictVectorizer()\n",
    "print(onehot_encoder.fit_transform(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder()\n",
    "X=[\n",
    "    {'city':1},\n",
    "    {'city':4},\n",
    "    {'city':5}\n",
    "]\n",
    "X = pd.DataFrame(X)\n",
    "print(onehot_encoder.fit_transform(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([\n",
    "    [0., 0., 5., 13., 9., 1.],\n",
    "    [0., 0., 13., 15., 10., 15.],\n",
    "    [0., 3., 15., 2., 0., 11.]\n",
    "])\n",
    "s = preprocessing.StandardScaler()\n",
    "print(s.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RobustScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class RobustScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n",
      " |  \n",
      " |  Scale features using statistics that are robust to outliers.\n",
      " |  \n",
      " |  This Scaler removes the median and scales the data according to\n",
      " |  the quantile range (defaults to IQR: Interquartile Range).\n",
      " |  The IQR is the range between the 1st quartile (25th quantile)\n",
      " |  and the 3rd quartile (75th quantile).\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by\n",
      " |  computing the relevant statistics on the samples in the training\n",
      " |  set. Median and interquartile range are then stored to be used on\n",
      " |  later data using the ``transform`` method.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators. Typically this is done by removing the mean\n",
      " |  and scaling to unit variance. However, outliers can often influence the\n",
      " |  sample mean / variance in a negative way. In such cases, the median and\n",
      " |  the interquartile range often give better results.\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  with_centering : boolean, True by default\n",
      " |      If True, center the data before scaling.\n",
      " |      This will cause ``transform`` to raise an exception when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_scaling : boolean, True by default\n",
      " |      If True, scale the data to interquartile range.\n",
      " |  \n",
      " |  quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0\n",
      " |      Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR\n",
      " |      Quantile range used to calculate ``scale_``.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  copy : boolean, optional, default is True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  center_ : array of floats\n",
      " |      The median value for each feature in the training set.\n",
      " |  \n",
      " |  scale_ : array of floats\n",
      " |      The (scaled) interquartile range for each feature in the training set.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import RobustScaler\n",
      " |  >>> X = [[ 1., -2.,  2.],\n",
      " |  ...      [ -2.,  1.,  3.],\n",
      " |  ...      [ 4.,  1., -2.]]\n",
      " |  >>> transformer = RobustScaler().fit(X)\n",
      " |  >>> transformer\n",
      " |  RobustScaler()\n",
      " |  >>> transformer.transform(X)\n",
      " |  array([[ 0. , -2. ,  0. ],\n",
      " |         [-1. ,  0. ,  0.4],\n",
      " |         [ 1. ,  0. , -1.6]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  robust_scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`sklearn.decomposition.PCA`\n",
      " |      Further removes the linear correlation across features with\n",
      " |      'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/Median\n",
      " |  https://en.wikipedia.org/wiki/Interquartile_range\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RobustScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the median and quantiles to be used for scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to compute the median and quantiles\n",
      " |          used for later scaling along the features axis.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like\n",
      " |          The data used to scale along the specified axis.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Center and scale the data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}\n",
      " |          The data used to scale along the specified axis.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessing.RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.         -1.6         0.          0.         -1.42857143]\n",
      " [ 0.          0.          0.          0.30769231  0.2         0.57142857]\n",
      " [ 0.          2.          0.4        -1.69230769 -1.8         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "s = RobustScaler()\n",
    "print(s.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"UNC played Duke in basketball\",\n",
    "    \"Duke lost the basketball game\"\n",
    "]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "# [[1 1 0 1 0 1 0 1]\n",
    "# [1 1 1 0 1 0 1 0]]\n",
    "print(vectorizer.vocabulary_)\n",
    "# {'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 1 0 0 1]\n",
      " [0 0 0 0 1 1 1 0 1 0 0 1 0]\n",
      " [1 1 1 1 0 0 0 0 0 0 1 0 0]]\n",
      "{'unc': 12, 'played': 9, 'duke': 5, 'in': 7, 'basketball': 4, 'lost': 8, 'the': 11, 'game': 6, 'ate': 3, 'sandwich': 10, 'and': 1, 'an': 0, 'apple': 2}\n"
     ]
    }
   ],
   "source": [
    "corpus.append(\"I ate a sandwich and an apple\")\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "# [[0 0 0 0 1 1 0 1 0 1 0 0 1]\n",
    "#  [0 0 0 0 1 1 1 0 1 0 0 1 0]\n",
    "#  [1 1 1 1 0 0 0 0 0 0 1 0 0]]\n",
    "print(vectorizer.vocabulary_)\n",
    "# {'unc': 12, 'played': 9, 'duke': 5, 'in': 7, \n",
    "#  'basketball': 4, 'lost': 8, 'the': 11, 'game': 6, \n",
    "#  'ate': 3, 'sandwich': 10, 'and': 1, 'an': 0, 'apple': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance between doc1 and doc2  [[2.44948974]]\n",
      "distance between doc1 and doc3  [[3.16227766]]\n",
      "distance between doc2 and doc3  [[3.16227766]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = vectorizer.fit_transform(corpus).todense()\n",
    "print(\"distance between doc1 and doc2 \", euclidean_distances(X[0],X[1]))\n",
    "print(\"distance between doc1 and doc3 \", euclidean_distances(X[0],X[2]))\n",
    "print(\"distance between doc2 and doc3 \", euclidean_distances(X[1],X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 1]\n",
      " [0 0 1 1 1 1 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0]]\n",
      "{'unc': 8, 'played': 6, 'duke': 3, 'basketball': 2, 'lost': 5, 'game': 4, 'ate': 1, 'sandwich': 7, 'apple': 0}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "# [[0 0 1 1 0 0 1 0 1]\n",
    "#  [0 0 1 1 1 1 0 0 0]\n",
    "#  [1 1 0 0 0 0 0 1 0]]\n",
    "print(vectorizer.vocabulary_)\n",
    "# {'unc': 8, 'played': 6, 'duke': 3, 'basketball': 2, \n",
    "# 'lost': 5, 'game': 4, 'ate': 1, 'sandwich': 7, 'apple': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "# [[1 0 1 0]\n",
    "# [0 1 0 1]]\n",
    "print(vectorizer.vocabulary_)\n",
    "# {'ate': 0, 'sandwiches': 2, 'sandwishes': 3, 'eaten': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n",
      "gathering\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'I am gathering ingredients for the sandwich.',\n",
    "    'There were many peoples at the gathering.'\n",
    "]\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# help(WordNetLemmatizer)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('gathering','v')) # gather，动词\n",
    "print(lemmatizer.lemmatize('gathering','n')) # gathering，名词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# help(PorterStemmer)\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('gathering')) # gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词干： [['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize # 取词\n",
    "from nltk.stem import PorterStemmer # 词干提取\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # 词性还原\n",
    "from nltk import pos_tag # 词性标注\n",
    "\n",
    "wordnet_tags = ['n','v']\n",
    "corpus = [\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "stemmer = PorterStemmer()\n",
    "print(\"词干：\", [[stemmer.stem(word) for word in word_tokenize(doc)] for doc in corpus])\n",
    "\n",
    "# 词干： [['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('He', 'PRP'), ('ate', 'VBD'), ('the', 'DT'), ('sandwiches', 'NNS')], [('Every', 'DT'), ('sandwich', 'NN'), ('was', 'VBD'), ('eaten', 'VBN'), ('by', 'IN'), ('him', 'PRP')]]\n",
      "词性还原： [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(word, tag):\n",
    "    if tag[0].lower() in ['n','v']:\n",
    "        return lemmatizer.lemmatize(word, tag[0].lower())\n",
    "    return word\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(doc)) for doc in corpus]\n",
    "\n",
    "print(tagged_corpus)\n",
    "# [[('He', 'PRP'), ('ate', 'VBD'), ('the', 'DT'), ('sandwiches', 'NNS')], \n",
    "#  [('Every', 'DT'), ('sandwich', 'NN'), ('was', 'VBD'), \n",
    "#   ('eaten', 'VBN'), ('by', 'IN'), ('him', 'PRP')]]\n",
    "\n",
    "print('词性还原：',[[lemmatize(word,tag) for word, tag in doc] for doc in tagged_corpus])\n",
    "# 词性还原： [['He', 'eat', 'the', 'sandwich'], \n",
    "#            ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
